# -*- coding: utf-8 -*-
"""Paris Housing Price (SVR).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b3LkMe3bXCQIaTt6OcprYrWPdPahpDwq

##Paris Housing Price Prediction using SVM Regression with Hyper-Parameter Tuning
###This project aims to predict housing prices in Paris by implementing Support Vector Machine (SVM) Regression, a powerful supervised machine learning algorithm. SVM is known for its ability to handle high-dimensional data and is effective in both classification and regression tasks. For this project, we'll focus on optimizing the SVM model for regression to capture complex relationships within the dataset's features and improve prediction accuracy.

###The dataset, sourced from Kaggle, provides various details about Parisian houses, including variables such as square footage, location specifics, and amenities. The primary objective here is to leverage SVM's kernel functions and fine-tune hyper-parameters such as C, epsilon, and the kernel type to minimize prediction errors and enhance model performance.

###Throughout this notebook, you will find sections covering:

* Data Preprocessing: Handling missing values, scaling, and encoding features if needed to ensure the dataset is compatible with the SVM model.
* Exploratory Data Analysis (EDA): Understanding the distribution of housing prices and feature relationships.
* Model Implementation: Building the SVM regression model and optimizing hyper-parameters using GridSearchCV for better accuracy and generalizability.
* Evaluation: Using metrics such as Mean Absolute Error (MAE) and Mean Squared Error (MSE) to evaluate the tuned model.

###By the end of this project, the goal is to establish a model capable of accurate and reliable price predictions, valuable for potential applications in real estate analytics in urban environments like Paris.
"""

# Data manipulation and visualization
import pandas as pd # It imports the pandas library and assigns it the alias "pd" for easier use.
import numpy as np # It brings in a tool called numpy, nicknamed "np", to help with number crunching in your code.
import matplotlib.pyplot as plt  # Import libraries for creating visualizations, aliased as plt.
import seaborn as sns # Import libraries for creating visualizations, aliased as sns.

# Machine learning and model evaluation
from sklearn.model_selection import train_test_split, GridSearchCV # It imports functions for splitting data and tuning model hyperparameters for better performance evaluation.
from sklearn.svm import SVR # It imports the Support Vector Regression (SVR) model for performing regression tasks using support vector machines.
from sklearn.preprocessing import StandardScaler # It imports the StandardScaler to standardize features by removing the mean and scaling to unit variance.
from sklearn.metrics import mean_absolute_error, mean_squared_error # It imports functions to calculate Mean Absolute Error (MAE) and Mean Squared Error (MSE) for regression model evaluation.

df = pd.read_csv("ParisHousing..csv") #Loads the data from the CSV file into a pandas DataFrame named df.

df.head() #Displays the first 5 rows of the DataFrame df.

df.info() # It prints a concise summary of the DataFrame, including data types, non-null values, and memory usage.

df.isnull().sum() #It calculates and displays the total number of missing values (NaN) in each column of the DataFrame.

"""###Here we can see that there is no null value in the dataframe."""

# Separate features and target variable
X = df.drop('price', axis=1)
y = df['price']
# It separates the dataset into features (X) by excluding the 'price' column and assigns 'price' as the target variable (y).

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# It splits the data into training and testing sets (80% train, 20% test) with a fixed random seed for reproducibility.

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# It standardizes the features (X_train and X_test) using the StandardScaler for better model performance.

# Define the SVM model
svm = SVR()
# It creates an instance of the Support Vector Regression (SVR) model with default parameters.

# Set up the hyper-parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],            # Regularization parameter
    'epsilon': [0.01, 0.1, 0.5, 1],    # Controls tolerance for errors
    'kernel': ['linear', 'poly', 'rbf'] # Kernel type
}

# Grid Search with cross-validation
grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters
print("Best Parameters:", grid_search.best_params_)

# Train the model with best parameters
best_svm = grid_search.best_estimator_
best_svm.fit(X_train, y_train)

# Make predictions
y_pred_train = best_svm.predict(X_train)
y_pred_test = best_svm.predict(X_test)

# Evaluation Metrics
train_mae = mean_absolute_error(y_train, y_pred_train)
test_mae = mean_absolute_error(y_test, y_pred_test)

train_mse = mean_squared_error(y_train, y_pred_train)
test_mse = mean_squared_error(y_test, y_pred_test)

print(f"Training MAE: {train_mae}")
print(f"Testing MAE: {test_mae}")
print(f"Training MSE: {train_mse}")
print(f"Testing MSE: {test_mse}")

# Plot predicted vs actual prices
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_test, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted Prices")
plt.show()

from sklearn.metrics import r2_score
rmse = np.sqrt(test_mse)
r2 = r2_score(y_test, y_pred_test)

print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (RÂ²): {r2}")

